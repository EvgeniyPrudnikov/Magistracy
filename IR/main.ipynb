{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "text_filename = './data/cran.all.1400'\n",
    "query_filename = './data/cran.qry'\n",
    "output_filename = './data/answer'\n",
    "\n",
    "with open(text_filename,'r') as f:\n",
    "    text = f.read()\n",
    "with open(query_filename,'r') as f:\n",
    "    queries = f.read()\n",
    "\n",
    "    \n",
    "def clearWPart(s, source, target):\n",
    "    ind = s.find(source) + len(source)\n",
    "    return s[:ind] + s[ind:].replace(source,target)\n",
    "    \n",
    "    \n",
    "def text_normalization(dirty_text):\n",
    "    clear_regexp = r\"\\W+\"\n",
    "    c_words = re.sub(clear_regexp, ' ', dirty_text, flags=re.DOTALL).strip().split()\n",
    "    stemmer = PorterStemmer()\n",
    "    filtered_words = [word for word in c_words if word not in stopwords.words('english')]\n",
    "    return [stemmer.stem(i) for i in filtered_words]\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    regex = r\"^\\.[ITW](.|\\n)*?(?=\\n\\.[AB]|\\Z)\"\n",
    "    matches = re.finditer(regex, text, re.MULTILINE)\n",
    "    f_text = ' '.join([e.group() for e in matches])\n",
    "    f_text = '.I'.join([clearWPart(i,'.W','') for i in f_text.split('.I')])\n",
    "    docs = re.split('.I|.T|.W', f_text)\n",
    "    t_docs = [[docs[i], docs[i + 1], docs[i + 2]] for i in xrange(1, len(docs)-2, 3)]\n",
    "    \n",
    "    c_docs = map(lambda r: (int(r[0].strip(' \\t\\n\\r')),\\\n",
    "                       text_normalization(r[1]),\\\n",
    "                       text_normalization(r[2])),\\\n",
    "            t_docs)\n",
    "    return c_docs\n",
    "\n",
    "\n",
    "def parse_queries(queries):\n",
    "    qrs = re.split('.I|.W', queries)\n",
    "    t_qrs = [[qrs[i], qrs[i + 1]] for i in xrange(1, len(qrs)-1, 2)]\n",
    "    c_qrs = map(lambda r: (int(r[0].strip(' \\t\\n\\r')),text_normalization(r[1])),t_qrs)\n",
    "    #z_qrs = map(lambda r: zip([r[0]]*len(r[1]),r[1]),c_qrs)\n",
    "    return c_qrs\n",
    "\n",
    "\n",
    "docs = parse_text(text)\n",
    "qrs = parse_queries(queries)\n",
    "\n",
    "\n",
    "def create_ivn_index(data, part='doc'):\n",
    "    idx = 3 if part == 'doc' else 1\n",
    "    wc_data = map(lambda r: (r[0],  [(k,v) for k,v in Counter(r[1]).iteritems()],len(r[1]),\\\n",
    "                                    [(k,v) for k,v in Counter(r[2]).iteritems()],len(r[2])),\n",
    "                  data)\n",
    "    p_data =  map(lambda r: zip([r[0]]*len(r[idx]),\n",
    "                                r[idx],\n",
    "                                [r[idx + 1]]*len(r[idx])), \n",
    "                  wc_data)\n",
    "    \n",
    "    flatted_data = [item for sublist in p_data for item in sublist] \n",
    "    index = defaultdict(list)\n",
    "    for k, v, doc_l in flatted_data:\n",
    "        index[v[0]].append((k,v[1],doc_l))\n",
    "    index['__metadata__'] = {'num_of_docs':len(data),\n",
    "                             'avgdl':np.average([len(doc[idx]) for doc in wc_data]),\n",
    "                             'index_len':len(index),\n",
    "                             'avg_docs_len':np.average([len(v) for k,v in index.iteritems()]),\n",
    "                             'max_docs_len':np.max([len(v) for k,v in index.iteritems()])}\n",
    "    return index\n",
    "\n",
    "\n",
    "doc_index = create_ivn_index(docs)\n",
    "\n",
    "title_index = create_ivn_index(docs, part='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_query(index, query):\n",
    "    \n",
    "    def score_BM25(N, nq, ft, D, avgdl):\n",
    "        k1 = 1.2\n",
    "        b = 0.75\n",
    "        #k2 = 0.01\n",
    "        def IDF(N, nq):\n",
    "            return log(1 + (N - nq + 0.5)/(nq + 0.5))\n",
    "    \n",
    "        return IDF(N,nq) * (ft*(k1 + 1))/(ft + k1*(1 - b + b * (D/avgdl))) #* ((k2 + 1)*ft/(k2 + ft)\n",
    "\n",
    "    num_docs_in_coll = index['__metadata__']['num_of_docs']\n",
    "    avgdl = index['__metadata__']['avgdl']\n",
    "    \n",
    "    query_result = dict()\n",
    "    for term in query:\n",
    "        if term in index:\n",
    "            term_docs = index[term]\n",
    "            for docid, freq, doc_len in term_docs:\n",
    "                score = score_BM25(N=num_docs_in_coll, nq=len(term_docs),ft=freq,D=doc_len, avgdl=avgdl)\n",
    "                if docid in query_result:\n",
    "                    query_result[docid] += score\n",
    "                else:\n",
    "                    query_result[docid] = score\n",
    "    return query_result\n",
    "\n",
    "\n",
    "def search_queries(index, queries, file_name=None):\n",
    "    if file_name:\n",
    "        f = open(file_name,'w')\n",
    "    for i,q in enumerate(queries):\n",
    "        query_res = process_query(index, q[1])\n",
    "        for doc_id in sorted(query_res, key=query_res.get, reverse=True)[:10]:\n",
    "            if file_name:\n",
    "                f.write(str(i+1) + ' ' + str(doc_id) + '\\n')\n",
    "            else:\n",
    "                print i + 1, doc_id\n",
    "    if file_name:\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "search_queries(doc_index, qrs, file_name='./data/answer_doc')\n",
    "search_queries(title_index, qrs, file_name='./data/answer_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean precision: 0.22024691358\n",
      "mean recall: 0.306024492383\n",
      "mean F-measure: 0.256145210109\n",
      "MAP@10: 0.21955973307\n",
      "\n",
      "\n",
      "mean precision: 0.173333333333\n",
      "mean recall: 0.246569763465\n",
      "mean F-measure: 0.203564866877\n",
      "MAP@10: 0.170676459926\n"
     ]
    }
   ],
   "source": [
    "groundtruth_file = './data/test.qrel_clean'\n",
    "title_answer_file = './data/answer_title'\n",
    "doc_answer_file = './data/answer_doc'\n",
    "\n",
    "def res_eval(groundtruth_file, answer_file):\n",
    "    q2reld = {} \n",
    "    for line in open(groundtruth_file):\n",
    "        qid, did = [int(x) for x in line.split()]\n",
    "        if qid in q2reld.keys():\n",
    "            q2reld[qid].add(did)\n",
    "        else:\n",
    "            q2reld[qid] = set()\n",
    "\n",
    "    q2retrd = {}\n",
    "    for line in open(answer_file):\n",
    "        qid, did = [int(x) for x in line.split()]\n",
    "        if qid in q2retrd.keys():\n",
    "            q2retrd[qid].append(did)\n",
    "        else:\n",
    "            q2retrd[qid] = []    \n",
    "\n",
    "\n",
    "    N = len(q2retrd.keys())\n",
    "    precision = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2retrd[q]) for q in q2retrd.keys()]) / N\n",
    "    recall = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2reld[q]) for q in q2retrd.keys()]) / N\n",
    "    print \"mean precision: {}\\nmean recall: {}\\nmean F-measure: {}\".format(precision, recall, 2*precision*recall/(precision+recall)) \n",
    "\n",
    "    # MAP@10\n",
    "    import numpy as np\n",
    "\n",
    "    MAP = 0.0\n",
    "    for q in q2retrd.keys():\n",
    "        n_results = min(10, len(q2retrd[q]))\n",
    "        avep = np.zeros(n_results)\n",
    "        for i in range(n_results):\n",
    "            avep[i:] += q2retrd[q][i] in q2reld[q]\n",
    "            avep[i] *= (q2retrd[q][i] in q2reld[q]) / (i+1.0)\n",
    "        MAP += sum(avep) / min(n_results, len(q2reld[q]))\n",
    "    print \"MAP@10: {}\".format(MAP/N) \n",
    "\n",
    "res_eval(groundtruth_file,doc_answer_file)\n",
    "print '\\n'\n",
    "res_eval(groundtruth_file,title_answer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
